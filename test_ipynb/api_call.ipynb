{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6658c8",
   "metadata": {},
   "source": [
    "# è¯¥è„šæœ¬ç”¨æ¥æµ‹è¯•é“¾æ¥é­”æ­apiæœåŠ¡æ˜¯å¦é€šç•…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97163dd1",
   "metadata": {},
   "source": [
    "## é€šè¿‡load dotenvåŠ è½½ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b076350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abfaea1",
   "metadata": {},
   "source": [
    "## æŸ¥çœ‹å¯¼å…¥çš„ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97746f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c80d92cf-e029-4240-acd1-dd89b92f5137\n",
      "https://api-inference.modelscope.cn/v1/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "print(os.environ.get(\"OPENAI_BASE_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81a748",
   "metadata": {},
   "source": [
    "## è°ƒç”¨æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "161d8a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ Python å®ç°çš„å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰ç®—æ³•ï¼Œé‡‡ç”¨ **åˆ—è¡¨æ¨å¯¼å¼** çš„æ–¹å¼ç¼–å†™ï¼Œä»£ç ç®€æ´ä¸”æ˜“äºç†è§£ï¼Œé€‚åˆæ•™å­¦å’Œåˆå­¦è€…å‚è€ƒã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰Python å®ç°\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr  # åŸºæœ¬æƒ…å†µï¼šæ•°ç»„é•¿åº¦ä¸º0æˆ–1æ—¶å·²æœ‰åº\n",
      "\n",
      "    # é€‰æ‹©ä¸­é—´å…ƒç´ ä½œä¸ºåŸºå‡†å€¼ï¼ˆpivotï¼‰\n",
      "    pivot = arr[len(arr) // 2]\n",
      "\n",
      "    # å°†æ•°ç»„åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š\n",
      "    # - å°äº pivot çš„å…ƒç´ \n",
      "    # - ç­‰äº pivot çš„å…ƒç´ ï¼ˆç”¨äºå¤„ç†é‡å¤å€¼ï¼‰\n",
      "    # - å¤§äº pivot çš„å…ƒç´ \n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "\n",
      "    # é€’å½’æ’åºå·¦å³éƒ¨åˆ†ï¼Œå¹¶å°†ç»“æœåˆå¹¶\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# æµ‹è¯•ç”¨ä¾‹\n",
      "if __name__ == \"__main__\":\n",
      "    arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "    sorted_arr = quicksort(arr)\n",
      "    print(\"æ’åºç»“æœ:\", sorted_arr)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ§  ç®—æ³•è¯´æ˜\n",
      "\n",
      "- **åˆ†æ²»ç­–ç•¥**ï¼šå°†æ•°ç»„åˆ’åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼ˆå°äºã€ç­‰äºã€å¤§äºåŸºå‡†å€¼ï¼‰ï¼Œé€’å½’æ’åºå·¦å³éƒ¨åˆ†ã€‚\n",
      "- **åŸºå‡†å€¼é€‰æ‹©**ï¼šé€‰æ‹©æ•°ç»„ä¸­é—´å…ƒç´ ä½œä¸º pivotï¼Œæœ‰åŠ©äºåœ¨å¤šæ•°æƒ…å†µä¸‹ä¿æŒå¹³è¡¡åˆ’åˆ†ã€‚\n",
      "- **æ—¶é—´å¤æ‚åº¦**ï¼š\n",
      "  - å¹³å‡æƒ…å†µï¼š`O(n log n)`\n",
      "  - æœ€åæƒ…å†µï¼ˆå¦‚æ¯æ¬¡é€‰æ‹©æœ€å·®çš„ pivotï¼‰ï¼š`O(n^2)`ï¼Œä½†å¯ä»¥é€šè¿‡éšæœºé€‰æ‹© pivot æ”¹å–„ã€‚\n",
      "- **ç©ºé—´å¤æ‚åº¦**ï¼š`O(n)`ï¼Œå› ä¸ºä½¿ç”¨äº†é¢å¤–çš„åˆ—è¡¨å­˜å‚¨ `left`ã€`middle` å’Œ `right`ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ” å¯é€‰æ”¹è¿›ï¼šåŸåœ°æ’åºç‰ˆæœ¬\n",
      "\n",
      "å¦‚æœä½ æƒ³å®ç° **åŸåœ°æ’åºï¼ˆin-placeï¼‰** çš„å¿«é€Ÿæ’åºï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼ Lomuto æˆ– Hoare åˆ†åŒºçš„æ–¹å¼ã€‚è¯¥å®ç°æ›´é«˜æ•ˆï¼Œä½†ä»£ç ç•¥å¤æ‚ã€‚å¦‚æœä½ éœ€è¦è¿™ä¸ªç‰ˆæœ¬ï¼Œä¹Ÿå¯ä»¥ç»§ç»­æé—®ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Œ ç¤ºä¾‹è¾“å‡º\n",
      "\n",
      "è¿è¡Œä¸Šè¿°ä»£ç åï¼Œè¾“å‡ºä¸ºï¼š\n",
      "\n",
      "```\n",
      "æ’åºç»“æœ: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "è¿™ä¸ªå®ç°éå¸¸é€‚åˆç”¨äºæ•™å­¦å’Œç†è§£å¿«é€Ÿæ’åºçš„åŸºæœ¬æ€æƒ³ã€‚å¦‚æœä½ å¸Œæœ›è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¦‚éšæœºé€‰æ‹© pivotã€åŸåœ°æ’åºç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"c80d92cf-e029-4240-acd1-dd89b92f5137\")\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"random_float\",\n",
    "            \"description\": \"get a random float\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": [],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-235B-A22B\",  # ModleScope Model-Id\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"ç”Ÿæˆä¸€ä¸ªéšæœºfloat\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    extra_body={\n",
    "        \"enable_thinking\": False,\n",
    "    },\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x7f47426c2ba0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe301729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "import os\n",
    "\n",
    "\n",
    "lite = LiteLlm(\n",
    "    model=\"openai/Qwen/Qwen3-235B-A22B\",\n",
    "    api_base=os.environ.get(\"OPENAI_BASE_URL\"),\n",
    "    stream=True,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "# lite.llm_client.chat()\n",
    "\n",
    "# openai call\n",
    "response = completion(\n",
    "    stream=True,\n",
    "    model=\"openai/Qwen/Qwen3-235B-A22B\",\n",
    "    llm_provider=\"openai\",\n",
    "    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n",
    "    extra_body={\n",
    "        \"enable_thinking\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d646a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x7f758e2c6ad0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.fetch_sync_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1a53831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f441f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "res = litellm.completion(\n",
    "    model=\"openai/Qwen/Qwen3-235B-A22B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": '\\n ä½ æ˜¯ä¸€ä¸ªè´Ÿè´£å®¡æ ¸æ–‡æ¡ˆæ˜¯å¦åŒ…å«æ•æ„Ÿè¯çš„agentï¼Œè¯·æ£€æŸ¥è¾“å…¥çš„æ–‡æ¡ˆæ˜¯å¦åŒ…å«æ•æ„Ÿè¯ï¼Œ\\n å¦‚æœåŒ…å«æ•æ„Ÿè¯ï¼Œè¯·è¿”å›æ•æ„Ÿè¯ï¼Œå¹¶è§£é‡Šï¼Œå¦åˆ™è¿”å›\"no sensitive word\"ã€‚\\n    è¯·å…³æ³¨å¦‚ä¸‹ä½†ä¸ä»…é™äºä»¥ä¸‹çš„æ•æ„Ÿè¯ï¼š\\n    1. æ”¿æ²»æ•æ„Ÿè¯\\n    2. å®—æ•™æ•æ„Ÿè¯\\n    3. æš´åŠ›æ•æ„Ÿè¯\\n    4. æš´åŠ›æå“æ•æ„Ÿè¯\\n    5. è‰²æƒ…æ•æ„Ÿè¯\\n    6. èˆ†è®ºå¯¹ç«‹æ•æ„Ÿè¯\\n å¦‚æœè¾“å…¥çš„æ–‡æ¡ˆä¸­ä¸åŒ…å«æ•æ„Ÿè¯ï¼Œé‚£ä¹ˆè¯·è¿”å›\"no sensitive word\"ã€‚\\n\\n\\nYou are an agent. Your internal name is \"sensitive_word\".\\n\\n The description about you is \"ä¸€ä¸ªè´Ÿè´£æ•æ„Ÿè¯æ£€æµ‹çš„agent\"\\n\\n\\nYou have a list of other agents to transfer to:\\n\\n\\nAgent name: root_agent\\nAgent description: ä¸€ä¸ªå¹¿å‘Šä»¥åŠæ–‡æ¡ˆçš„å†…å®¹å®¡æ ¸åŠ©æ‰‹\\n\\n\\nAgent name: wrong_word\\nAgent description: ä¸€ä¸ªè´Ÿè´£æ–‡æ¡ˆé”™åˆ«å­—æ£€æµ‹çš„agent\\n\\n\\nIf you are the best to answer the question according to your description, you\\ncan answer it.\\n\\nIf another agent is better for answering the question according to its\\ndescription, call `transfer_to_agent` function to transfer the\\nquestion to that agent. When transferring, do not generate any text other than\\nthe function call.\\n\\nYour parent agent is root_agent. If neither the other agents nor\\nyou are best for answering the question according to the descriptions, transfer\\nto your parent agent. If you don\\'t have parent agent, try answer by yourself.\\n',\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"æ¬§è±é›…æ–°å“å‘å”®\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"For context:\"},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"[root_agent] called tool `transfer_to_agent` with parameters: {'agent_name': 'sensitive_word'}\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"For context:\"},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"[root_agent] `transfer_to_agent` tool returned result: {}\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"transfer_to_agent\",\n",
    "                \"description\": \"Transfer the question to another agent.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\"agent_name\": {\"type\": \"string\"}},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    api_base=None,\n",
    "    api_key=\"ee879477-517c-4899-9793-de2b20aad7ba\",\n",
    "    extra_body={\"enable_thinking\": True, \"stream\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47935f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34866a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper at 0x7f135fa282d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = litellm.completion(\n",
    "    model=\"openai/Qwen/Qwen3-235B-A22B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": 'You are an expert delegator that can delegate the user request to the\\nappropriate remote agents.\\n\\nTaskDescription:\\nä½ éœ€è¦ä½¿ç”¨ä½ çš„agentsæ¥æ‰§è¡Œæ–‡æ¡ˆå®¡æ ¸çš„å†…å®¹ï¼ŒåŒæ—¶ä½ éœ€è¦ä½¿ç”¨å®ƒä»¬çš„è¿”å›ç»“æœå¹¶æœ€ç»ˆæ€»è¿”å›ç»“æœ ï¼Œ\\nå¦‚æœå¯ä»¥è¯·åŒæ—¶è°ƒç”¨æ‰€æœ‰çš„agents\\n\\n\\nDiscovery:\\n- You can use `list_agents` to list the available remote agents you\\ncan use to delegate the task.\\n\\nExecution:\\n- For actionable requests, you can use `send_message` to interact with remote agents to take action.\\n- åœ¨æ€»ç»“ä¹‹å‰ä½ éœ€è¦è€ƒè™‘æ˜¯å¦å·²ç»å®Œå…¨è°ƒç”¨äº†ä½ çš„æ‰€æœ‰agents\\n\\nBe sure to include the remote agent name when you respond to the user.\\n\\nPlease rely on tools to address the request, and don\\'t make up the response. If you are not sure, please ask the user for more details.\\nFocus on the most recent parts of the conversation primarily.\\n\\n\\n\\n\\n\\nYou are an agent. Your internal name is \"root_agent\".\\n\\n The description about you is \"ä¸€ä¸ªå¹¿å‘Šä»¥åŠæ–‡æ¡ˆçš„å†…å®¹å®¡æ ¸åŠ©æ‰‹\"\\n\\n\\nYou have a list of other agents to transfer to:\\n\\n\\nAgent name: sensitive_word\\nAgent description: ä¸€ä¸ªè´Ÿè´£æ•æ„Ÿè¯æ£€æµ‹çš„agent\\n\\n\\nAgent name: wrong_word\\nAgent description: ä¸€ä¸ªè´Ÿè´£æ–‡æ¡ˆé”™åˆ«å­—æ£€æµ‹çš„agent\\n\\n\\nIf you are the best to answer the question according to your description, you\\ncan answer it.\\n\\nIf another agent is better for answering the question according to its\\ndescription, call `transfer_to_agent` function to transfer the\\nquestion to that agent. When transferring, do not generate any text other than\\nthe function call.\\n',\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"æ¬§è±é›…æ–°å“å‘å”®\"},\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"transfer_to_agent\",\n",
    "                \"description\": \"Transfer the question to another agent.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\"agent_name\": {\"type\": \"string\"}},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    api_base=None,\n",
    "    api_key=\"ee879477-517c-4899-9793-de2b20aad7ba\",\n",
    "    extra_body={\"enable_thinking\": True, \"stream\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
